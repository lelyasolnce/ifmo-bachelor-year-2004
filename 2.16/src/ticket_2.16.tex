\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{ifpdf}

\newcommand{\mys}{\textit}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}\nolimits}

%%%% Changes quotation marks
\DeclareRobustCommand{\flqq}{\textormath{\guillemotleft}{\mbox{\guillemotleft}}}
\DeclareRobustCommand{\frqq}{\textormath{\guillemotright}{\mbox{\guillemotright}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\section*{Билет \No 2.16}
{\em Постановка и численное решение задач оптимизации. Целевая функция, ограничения. 
Спуск по координатам, наискорейший спуск, случайный поиск. Условная оптимизация.}

\section{Постановка задачи}
	Оптимизация занимается поиском наилучших решений. Критерий качества решения формулируется в виде функций многих переменных.
	Это \mys{целевая функция}. Переменные~-- \mys{параметры оптимизации}. Обычно имеются некоторые ограничения на эти переменные.
	Ограничения бывают двух типов: \mys{ограничения равенства} (с жёстким допуском) и \mys{ограничения-неравенства} (формулируются в виде диапазона).

	Задача оптмизации детерминированных систем:
	\begin{equation}
		\left\{
			\begin{aligned}
				f(x) &\rightarrow min \\
				c(x) &= 0 \\
				\overline{c}(x) &\ge 0
			\end{aligned}
		\right.
	\end{equation}

\section{Безусловная оптмимзация}
	\subsection{Покоординатный спуск}
		Алгоритм состоит в нахождении локальных минимумов последовательно в каждом направлении.
		Возьмём некоторую переменную $x_1$, а остальные зафиксируем. Найдём минимум $f$ по этой переменной. $x_1^{min}$~-- 1-я координата минимума.
		Заменим $x_1$ на $x_1^{min}$. Пререйдём к $x_2$. И т.д.

	\subsection{Наискорейший спуск}
		Этот метод предполагает, что функция $f$ непрерывна и имеет непрерывные первые производные.
		Задача нахождения минимума сводится к отысканию стационарной точки~$x^*$ ($\bigtriangledown f(x^*) = 0$).
	
		Алгоритм наискорейшего спуска состоит в следующем:
		\begin{enumerate}
			\item Выбрать начальную точку $x^0$.
			\item \label{fastit} На $k$-й итерации $d_k = -\bigtriangledown f(x^k)$; найти такое $\lambda_k \ge 0$, что
			  	  $$\lambda_k = \argmin g(\lambda) \mbox{, где } g(\lambda) = f(x^k + \lambda d_k) \mbox{ (одномерная минимизация)}.$$
			  	  Положить $x^{k+1} = x^k + \lambda_k d_k.$
			\item Провести тест на остановку. Если он выполнен, то закончить процедуру;
			  	  если нет, то перейти от $k$ к $k+1$ и перейти к шагу \ref{fastit}.
		\end{enumerate}

		В качестве теста на остановку можно взять, например, такой:
		$$\|\bigtriangledown f\|^2 \le \epsilon \mbox{ (при заданном заранее }\epsilon\mbox{)}.$$

	\subsection{Случайный поиск??}
			Т.к. что писать сюда непонятно, пусть будет алгоритм имитации отжига, который является вполне себе случайным.
			
			Алгоритм имитации отжига воспроизводит процесс кристаллизации, который происходит,
			например, при отжиге металлов (отсюда такое название метода).
			
			Решения задачи воспринимаются как состояния
			некоторой физической системы. Каждому состоянию $s$ сопоставляется энергия $E(s)$. Алгоритм в ходе работы
			ищет состояние системы с минимальной энергией. Он начинает с некоторого стартового состояния системы $s_0$.
			На каждом шаге выбирается новое состояние для перехода. Это делается случайным образом, причём обычно так,
			что вероятность состояния быть выбранным зависит от расстояния до текущего состояния. После того как для перехода из
			состояния $s_i$ было выбрано некоторое состояние $s_{i+1}$ рассчитывается величина $\Delta E = E(s_{i+1}) - E(s_i)$.
			Если она оказывается отрицательна, то система переходит в состояние $s_{i+1}$. Если же она положительна,
			то переход в $s_{i+1}$ делается с вероятностью $p = p(\Delta E, T)$, где $T$~-- это температура системы.
			В качестве $p(\Delta E, T)$ может быть взята, например, функция $e^{- \frac{\Delta E}{T}}$. 
			Температура системы постепенно понижается в ходе работы алгоритма.	

\section{Условная оптмимизация??}
	Обычно исходную функцию как-то модифицируют, чтобы получить задачу оптимизации без ограничений, но с решением,
	совпадающим с решением для исходной функции. Модифицированная функция – целевая функция (пример – минимизирующая функция Лагранжа).
	
	\subsection{Метод проекции градиента}
		Метод проекции градиента модифицирует процедуру наискорейшего спуска так, чтобы она учитывала наличие ограничений-неравенств. 
		Алгоритм состоит в следующем:
		\begin{enumerate}
			\item Выбрать начальную точку $x^0$.
			\item \label{projIt} На итерации $k$ текущая точка $x^k$. Определить множество индексов насыщенных ограничений $I^0$~-- 
				  тех, для которых в точке $x^k$ достигается равенство. Пусть $A^0$ матрица, 
				  строки которой соответствуют ограничениям из $I^0$. Вычислить матрицу проектирования 
				  $$P^0 = I - A^{0T} [A^0A^{0T}]^{-1} A^0.$$ Затем вычислить $d_k = - P^0 \bigtriangledown f(x^k).$
			\item \label{KunTackerCheck} Пусть $u = [A^0A^{0T}]^{-1} A^0 \bigtriangledown f(x^k)$. Если $u \ge 0$ то перейти к \ref{do}. 
				  Иначе, пусть $u_i$ наибольшая по модулю отрицательная компонента $u$. 
				  Тогда можно положить $I^0 = I^0 - \{i\}$ и вернуться к \ref{projIt}.	  
			\item \label{do} Если $d_k = 0$, то конец. Если $d_k \not = 0$ найти $\lambda_{max} = max\{\lambda | x^k + \lambda d_k \in X\}.$ 
				  Найти такое $\lambda_k \in [0, \lambda_{max}]$, что
				  $$\lambda_k = \argmin g(\lambda) \mbox{, где } g(\lambda) = f(x^k + \lambda d_k).$$
				  Перейти от $k$ к $k+1$ и перейти к шагу \ref{projIt}.	
		\end{enumerate}

\end{document}
